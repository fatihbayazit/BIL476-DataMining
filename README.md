# BIL476-DataMining
# Dimensionality Reduction Using the Forward-Forward Algorithm

This repository contains the implementation and experiments for the project **"Dimensionality Reduction Using the Forward-Forward Algorithm"**, conducted as part of the BİL476 / BİL573 course at TOBB University of Economics and Technology.

## 📌 Project Overview
The **Forward-Forward (FF)** algorithm, introduced by Geoffrey Hinton in 2022, is a biologically inspired alternative to the traditional **Backpropagation (BP)** algorithm for training neural networks. Instead of global gradient backpropagation, FF trains each layer independently by distinguishing between **positive** and **negative** samples using a **goodness** metric.

In this project, we investigate the applicability of the FF algorithm to **unsupervised learning**, specifically **dimensionality reduction** using **autoencoders**, and compare its performance against:
- Principal Component Analysis (**PCA**)
- Singular Value Decomposition (**SVD**)
- Standard BP-trained Autoencoders

## 📂 Dataset
Two benchmark datasets were used:
- **MNIST**: 28×28 grayscale handwritten digit images (60,000 train / 10,000 test samples).
- **Fashion-MNIST**: 28×28 grayscale images of 10 clothing categories (60,000 train / 10,000 test samples).

**Preprocessing:**
- Pixel values normalized to `[0,1]`.
- For **Unsupervised FF**, negative samples were generated by mixing two random images with a Gaussian blur mask.

## ⚙️ Methods
The following methods were implemented:
1. **Supervised FF Autoencoder** – negative samples generated via incorrect labels.
2. **Unsupervised FF Autoencoder** – negative samples generated via image mixing.
3. **BP Autoencoder** – trained using Mean Squared Error (MSE) loss.
4. **PCA** – linear dimensionality reduction.
5. **SVD** – matrix factorization-based dimensionality reduction.

**Common parameters:**
- Latent dimension: `78`
- Evaluation metric: **MSE**
- Framework: **PyTorch**
- Experiment tracking: **Weights & Biases**

## 📊 Results
**MNIST (MSE)**:
| Method           | MSE    |
|------------------|--------|
| BP Autoencoder   | 0.007  |
| Supervised FF    | 0.042  |
| Unsupervised FF  | 0.031  |
| PCA              | 0.006  |
| SVD              | 0.006  |

**Fashion-MNIST (MSE)**:
| Method           | MSE    |
|------------------|--------|
| BP Autoencoder   | 0.012  |
| Supervised FF    | 0.054  |
| Unsupervised FF  | 0.045  |
| PCA              | 0.008  |
| SVD              | 0.008  |

**Key observations:**
- BP and PCA achieved the lowest reconstruction errors.
- Unsupervised FF outperformed supervised FF in both datasets.
- FF-based autoencoders can produce meaningful reconstructions without backpropagation.

## 📌 Conclusion
- FF is **viable** for dimensionality reduction tasks.
- Unsupervised FF variant shows **better performance** than the supervised version.
- While BP remains more accurate, FF offers unique advantages such as **layer-wise training** and **biological plausibility**.

## 🚀 Future Work
- Explore deeper and convolutional FF architectures.
- Apply FF to generative and transfer learning tasks.
- Conduct statistical significance testing.

## 📎 References
1. Hinton, G. (2022). *The forward-forward algorithm: Some preliminary investigations*. arXiv:2212.13345.
2. LeCun, Y., et al. (1998). *Gradient-based learning applied to document recognition*. Proceedings of the IEEE.
3. Xiao, H., et al. (2017). *Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms*.
4. Reyes-Angulo, A. A., et al. (2024). *Forward-Forward Algorithm for Hyperspectral Image Classification*.

---

**Author:** Fatih Bayazıt  
**Course:** BİL476 / BİL573 – Data Mining / Machine Learning  
**Institution:** TOBB University of Economics and Technology
